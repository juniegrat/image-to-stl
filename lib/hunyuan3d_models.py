#!/usr/bin/env python3
"""
Gestion des mod√®les et pipelines pour Hunyuan3D-2mv
G√®re le chargement des mod√®les, v√©rification d'environnement et pipelines
"""

import torch
import warnings
from typing import Optional, Dict, Any, List
from pathlib import Path


class ModelManager:
    """
    Gestionnaire de mod√®les pour Hunyuan3D-2mv
    G√®re le chargement et la configuration des pipelines
    """

    def __init__(
        self,
        model_path: str = "tencent/Hunyuan3D-2",
        texture_model_path: str = "tencent/Hunyuan3D-2",
        device: Optional[str] = None,
        disable_texture: bool = False
    ):
        """
        Initialise le gestionnaire de mod√®les

        Args:
            model_path: Chemin vers le mod√®le de forme
            texture_model_path: Chemin vers le mod√®le de texture
            device: Device √† utiliser (auto-d√©tect√© si None)
            disable_texture: D√©sactiver compl√®tement le chargement du mod√®le de texture
        """
        self.model_path = model_path
        self.texture_model_path = texture_model_path
        self.device = device or self._detect_device()
        self.disable_texture = disable_texture

        # Pipelines (charg√©s √† la demande)
        self.shape_pipeline = None
        self.texture_pipeline = None
        self._models_loaded = False

        # Supprimer les warnings
        warnings.filterwarnings("ignore", category=UserWarning)
        warnings.filterwarnings("ignore", category=FutureWarning)

        print(f"ü§ñ Gestionnaire de mod√®les initialis√©")
        print(f"   Device: {self.device}")
        print(f"   Mod√®le forme: {self.model_path}")
        print(f"   Mod√®le texture: {self.texture_model_path}")

    def _detect_device(self) -> str:
        """D√©tecte automatiquement le meilleur device disponible"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    def check_environment(self) -> bool:
        """
        V√©rifie l'environnement et les d√©pendances

        Returns:
            True si l'environnement est OK, False sinon
        """
        print("üîç V√©rification de l'environnement...")

        # V√©rifier CUDA
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(
                0).total_memory / 1024**3
            print(f"‚úÖ CUDA: {gpu_name}")
            print(f"   M√©moire GPU: {gpu_memory:.1f} GB")
        else:
            print("‚ö†Ô∏è  CUDA non disponible, utilisation CPU")

        # V√©rifier les d√©pendances critiques
        deps = [
            'diffusers', 'transformers', 'accelerate', 'torch',
            'torchvision', 'PIL', 'numpy', 'trimesh', 'imageio',
            'rembg', 'tqdm', 'matplotlib'
        ]
        missing = []

        for dep in deps:
            try:
                if dep == 'PIL':
                    import PIL
                elif dep == 'matplotlib':
                    import matplotlib.pyplot as plt
                else:
                    __import__(dep)
            except ImportError:
                missing.append(dep)

        if missing:
            print(f"‚ùå D√©pendances manquantes: {', '.join(missing)}")
            return False

        print("‚úÖ Toutes les d√©pendances sont disponibles")

        # V√©rifier Hunyuan3D
        try:
            import hy3dgen
            print("‚úÖ Module hy3dgen d√©tect√©")
            return True
        except ImportError:
            print("‚ùå Module hy3dgen non trouv√©")
            print("üí° Ex√©cutez: python install-hunyuan3d.py")
            return False

    def load_models(self) -> bool:
        """
        Charge les mod√®les Hunyuan3D-2

        Returns:
            True si succ√®s, False sinon
        """
        if self._models_loaded:
            print("‚úÖ Mod√®les d√©j√† charg√©s")
            return True

        print("ü§ñ Chargement des mod√®les...")

        try:
            # Importer les classes n√©cessaires
            from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline
            from hy3dgen.texgen import Hunyuan3DPaintPipeline

            # Charger le mod√®le de forme
            success = self._load_shape_model(Hunyuan3DDiTFlowMatchingPipeline)
            if not success:
                return False

            # Charger le mod√®le de texture (optionnel)
            if not self.disable_texture:
                self._load_texture_model(Hunyuan3DPaintPipeline)
            else:
                print("   üö´ Chargement du mod√®le de texture d√©sactiv√©")
                self.texture_pipeline = None

            self._models_loaded = True
            print("‚úÖ Mod√®les charg√©s avec succ√®s!")
            return True

        except ImportError as e:
            print(f"‚ùå Erreur lors de l'importation des modules Hunyuan3D: {e}")
            print("üí° V√©rifiez que Hunyuan3D-2 est correctement install√©")
            print("üí° Ex√©cutez: python install-hunyuan3d.py")
            return False
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des mod√®les: {e}")
            return False

    def _load_shape_model(self, pipeline_class) -> bool:
        """Charge le mod√®le de forme"""
        print(f"   üìê Chargement du mod√®le de forme...")
        try:
            self.shape_pipeline = pipeline_class.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                cache_dir=None  # Utilise le cache HF standard
            )
            print(f"   ‚úÖ Mod√®le de forme charg√© avec succ√®s!")
            return True
        except Exception as e:
            print(f"   ‚ùå Erreur chargement mod√®le de forme: {e}")
            return False

    def _load_texture_model(self, pipeline_class) -> bool:
        """Charge le mod√®le de texture"""
        print("   üé® Chargement du mod√®le de texture...")
        try:
            # Essayer plusieurs chemins possibles pour le mod√®le de texture
            texture_paths = [
                self.texture_model_path,
                "tencent/Hunyuan3D-2",  # Utilise le cache HF standard
            ]

            texture_loaded = False
            for path in texture_paths:
                try:
                    print(f"      Tentative avec: {path}")
                    self.texture_pipeline = pipeline_class.from_pretrained(
                        path)

                    # D√©placer vers le device manuellement si n√©cessaire
                    if hasattr(self.texture_pipeline, 'to'):
                        self.texture_pipeline = self.texture_pipeline.to(
                            self.device)

                    print("   ‚úÖ Mod√®le de texture charg√© avec succ√®s!")
                    texture_loaded = True
                    break
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  √âchec avec {path}: {e}")
                    continue

            if not texture_loaded:
                print("   ‚ö†Ô∏è  Impossible de charger le mod√®le de texture")
                print("   üìã Mode disponible: g√©n√©ration de forme uniquement")
                self.texture_pipeline = None
                return False

            return True

        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur g√©n√©rale texture: {e}")
            print("   Continuation sans texture (mesh uniquement)")
            self.texture_pipeline = None
            return False

    def generate_3d_mesh(
        self,
        images: List,
        config: Dict[str, Any],
        progress_callback=None
    ):
        """
        G√©n√®re un mesh 3D √† partir des images

        Args:
            images: Liste des images pr√©par√©es
            config: Configuration des param√®tres
            progress_callback: Callback de progression (optionnel)

        Returns:
            Mesh 3D g√©n√©r√© ou None en cas d'erreur
        """
        if not self.shape_pipeline:
            print("‚ùå Mod√®le de forme non charg√©")
            return None

        try:
            # Pr√©parer le g√©n√©rateur
            generator = torch.Generator(
                device=self.device).manual_seed(config['seed'])

            # G√©n√©rer selon le nombre d'images
            if len(images) > 1:
                # Mode multi-view
                print(f"   üîÑ Mode multi-view avec {len(images)} images")
                mesh = self.shape_pipeline(
                    image=images,
                    guidance_scale=config['guidance_scale'],
                    num_inference_steps=config['num_inference_steps'],
                    octree_resolution=config['octree_resolution'],
                    num_chunks=config['num_chunks'],
                    generator=generator,
                    callback=progress_callback,
                    callback_steps=1 if progress_callback else None,
                    output_type='trimesh'
                )[0]
            else:
                # Mode single view
                print("   üîÑ Mode single view")
                mesh = self.shape_pipeline(
                    image=images[0],
                    guidance_scale=config['guidance_scale'],
                    num_inference_steps=config['num_inference_steps'],
                    octree_resolution=config['octree_resolution'],
                    num_chunks=config['num_chunks'],
                    generator=generator,
                    callback=progress_callback,
                    callback_steps=1 if progress_callback else None,
                    output_type='trimesh'
                )[0]

            # Statistiques du mesh
            print(
                f"   ‚úÖ Mesh g√©n√©r√©: {len(mesh.vertices)} vertices, {len(mesh.faces)} faces")
            return mesh

        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration mesh: {e}")
            return None

    def apply_texture(
        self,
        mesh,
        reference_image,
        config: Dict[str, Any]
    ):
        """
        Applique une texture au mesh

        Args:
            mesh: Mesh 3D de base
            reference_image: Image de r√©f√©rence pour la texture
            config: Configuration des param√®tres

        Returns:
            Mesh textur√©
        """
        if not self.texture_pipeline:
            print("‚ö†Ô∏è  Mod√®le de texture non charg√©, conservation du mesh sans texture")
            return mesh

        try:
            # Pr√©parer les param√®tres de texture
            texture_steps = config.get('texture_steps', 40)
            guidance_scale = config.get('texture_guidance_scale', 2.0)

            print(
                f"   üîÑ Application de texture ({texture_steps} steps, guidance={guidance_scale})...")

            # Appeler le pipeline de texture
            textured_mesh = self.texture_pipeline(
                mesh,
                image=reference_image,
                guidance_scale=guidance_scale,
                num_inference_steps=texture_steps
            )

            print("   ‚úÖ Texture appliqu√©e avec succ√®s")
            return textured_mesh

        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur application texture: {e}")
            print("   Retour au mesh sans texture")
            return mesh

    def unload_models(self):
        """D√©charge les mod√®les pour lib√©rer la m√©moire"""
        print("üóëÔ∏è  D√©chargement des mod√®les...")

        if self.shape_pipeline:
            del self.shape_pipeline
            self.shape_pipeline = None

        if self.texture_pipeline:
            del self.texture_pipeline
            self.texture_pipeline = None

        # Nettoyer le cache GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self._models_loaded = False
        print("‚úÖ Mod√®les d√©charg√©s")

    def get_model_info(self) -> Dict[str, Any]:
        """Retourne des informations sur les mod√®les charg√©s"""
        return {
            'model_path': self.model_path,
            'texture_model_path': self.texture_model_path,
            'device': self.device,
            'disable_texture': self.disable_texture,
            'models_loaded': self._models_loaded,
            'shape_pipeline_loaded': self.shape_pipeline is not None,
            'texture_pipeline_loaded': self.texture_pipeline is not None,
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }

    def get_memory_usage(self) -> Dict[str, Any]:
        """Retourne l'utilisation m√©moire actuelle"""
        memory_info = {
            'device': self.device,
            'models_loaded': self._models_loaded
        }

        if torch.cuda.is_available():
            memory_info.update({
                'gpu_memory_allocated': torch.cuda.memory_allocated() / 1024**3,  # GB
                'gpu_memory_reserved': torch.cuda.memory_reserved() / 1024**3,   # GB
                # GB
                'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**3
            })

        return memory_info

    def optimize_memory(self):
        """Optimise l'utilisation m√©moire"""
        print("üßπ Optimisation m√©moire...")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   ‚úÖ Cache GPU nettoy√©")

        # Garbage collection Python
        import gc
        gc.collect()
        print("   ‚úÖ Garbage collection effectu√©")

    def validate_installation(self) -> Dict[str, bool]:
        """Valide l'installation compl√®te de Hunyuan3D"""
        validation = {
            'environment_ok': False,
            'hy3dgen_available': False,
            'models_loadable': False,
            'cuda_available': torch.cuda.is_available()
        }

        # V√©rifier l'environnement
        validation['environment_ok'] = self.check_environment()

        # V√©rifier hy3dgen
        try:
            import hy3dgen
            validation['hy3dgen_available'] = True
        except ImportError:
            validation['hy3dgen_available'] = False

        # Tester le chargement des mod√®les (sans vraiment charger)
        try:
            from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline
            from hy3dgen.texgen import Hunyuan3DPaintPipeline
            validation['models_loadable'] = True
        except ImportError:
            validation['models_loadable'] = False

        return validation

    def get_info(self) -> Dict[str, Any]:
        """Retourne des informations compl√®tes sur le gestionnaire"""
        return {
            'name': 'Hunyuan3D Model Manager',
            'description': 'Gestionnaire de mod√®les et pipelines pour Hunyuan3D-2mv',
            'model_info': self.get_model_info(),
            'memory_info': self.get_memory_usage(),
            'validation': self.validate_installation(),
            'features': [
                'Chargement automatique des mod√®les',
                'Gestion m√©moire optimis√©e',
                'Support multi-device (CUDA/MPS/CPU)',
                'Validation d\'installation',
                'G√©n√©ration mesh et texture',
                'D√©chargement s√©lectif des mod√®les',
                'Monitoring m√©moire GPU',
                'Gestion d\'erreurs robuste'
            ],
            'functions': [
                'check_environment: V√©rification environnement',
                'load_models: Chargement des mod√®les',
                'generate_3d_mesh: G√©n√©ration de mesh 3D',
                'apply_texture: Application de texture',
                'unload_models: D√©chargement des mod√®les',
                'optimize_memory: Optimisation m√©moire',
                'validate_installation: Validation compl√®te'
            ]
        }


# Instance globale du gestionnaire
model_manager = ModelManager()


def get_model_manager() -> ModelManager:
    """Retourne l'instance globale du gestionnaire de mod√®les"""
    return model_manager


def check_environment() -> bool:
    """Fonction de convenance pour v√©rifier l'environnement"""
    return model_manager.check_environment()


def load_models(
    model_path: str = "tencent/Hunyuan3D-2",
    texture_model_path: str = "tencent/Hunyuan3D-2",
    device: Optional[str] = None,
    disable_texture: bool = False
) -> bool:
    """
    Fonction de convenance pour charger les mod√®les

    Args:
        model_path: Chemin vers le mod√®le de forme
        texture_model_path: Chemin vers le mod√®le de texture
        device: Device √† utiliser
        disable_texture: D√©sactiver le mod√®le de texture

    Returns:
        True si succ√®s, False sinon
    """
    global model_manager
    model_manager = ModelManager(
        model_path, texture_model_path, device, disable_texture)
    return model_manager.load_models()


def get_model_info() -> Dict[str, Any]:
    """Fonction de convenance pour obtenir les informations des mod√®les"""
    return model_manager.get_info()
